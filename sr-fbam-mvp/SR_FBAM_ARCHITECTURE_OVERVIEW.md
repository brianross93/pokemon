# SR-FBAM: High-Level Engineering & Architecture Overview

## Executive Summary

SR-FBAM (Symbolic-Recurrence Frame-Based Action Model) is a novel hybrid architecture that combines neural networks with discrete symbolic memory to solve long-horizon sequential tasks. It addresses critical limitations in traditional Frame-Based Action Models (FBAMs) by introducing sparse, associative memory and learned gating mechanisms.

## Core Problem Statement

Traditional FBAMs suffer from three critical limitations:
1. **O(n) Wall-Time**: Reprocessing every frame for long sequences leads to prohibitive computational costs
2. **Poor Generalization**: Dense soft-attention mechanisms fail to capture abstract, compositional structures that generalize across contexts
3. **Brittle Feature Extraction**: Hand-engineered or regex-based extractors are fragile and not language-agnostic

## Architecture Overview

### 1. Frame-Based Processing
- **Input**: 40x120 ASCII character grids representing code snapshots or game states
- **Intra-Frame Encoding**: Transformer Encoder processes current frame to extract salient features
- **Cross-Domain**: Same architecture works for both code editing and game control

### 2. Symbolic Memory (Core Innovation)
- **Sparse, Associative Recall**: O(1) hash-based lookups vs O(n) soft-attention
- **Discrete Operations**: ASSOC, FOLLOW, WRITE, HALT operations
- **Persistent Storage**: Entities and relationships stored across time
- **Language-Agnostic**: Works across programming languages and game domains

### 3. Transformer Symbol Extractor
- **Learned Extraction**: Replaces brittle regex-based extractors
- **Multi-Language**: Automatically adapts to Python, JavaScript, etc.
- **Game-Aware**: Extracts relevant game entities (Pokemon, areas, dialogue)

### 4. Memory Cross-Attention
- **Contextualization**: Relates current symbols to historical memory
- **Efficient**: Only processes relevant historical context
- **Scalable**: Memory size doesn't impact computation linearly

### 5. Selective Gating (ExtractionGate)
- **Learned Decision**: MLP decides between ENCODE (expensive) vs ASSOC (cheap)
- **Efficiency**: Avoids unnecessary computation
- **Supervised Learning**: Hindsight gate labels provide training signal

### 6. LSTM Integrator
- **Temporal State**: Maintains agent state across time
- **Recurrence-Completeness**: Supports unbounded sequential computations
- **Integration**: Combines frame, symbols, and memory information

## Pokemon Blue Integration

### 1. PyBoy Adapter
- **Game Boy Emulation**: Connects to PyBoy emulator
- **Memory Offsets**: Reads game state from specific RAM addresses
- **Visual/Headless**: Supports both visual and headless operation

### 2. Frame Encoder
- **State Conversion**: Transforms game telemetry to ASCII grid format
- **Unified Processing**: Same architecture as code editing
- **Numeric Features**: Augments visual with precise game state

### 3. LLM Integration (GPT-5)
- **High-Level Reasoning**: Corresponds to HALT operation in SR-FBAM
- **Asynchronous Plans**: LLM generates action sequences with TTL
- **Rare Invocation**: Only called when SR-FBAM cannot resolve situation

### 4. Deliberate Gating System
- **Badness Score**: Composite heuristic quantifying need for LLM intervention
- **Debounced State**: Ensures stable state changes before triggering
- **Cooldowns**: Prevents excessive LLM calls
- **Rate Limiting**: Maximum calls per minute

### 5. Deterministic Bootstrap
- **Intro Handling**: Scripted sequence for Pokemon Blue intro
- **Control Transfer**: Reliable handoff to SR-FBAM after intro
- **Softlock Prevention**: Avoids getting stuck in intro sequences

## Key Metrics & Results

### Efficiency Improvements
- **LLM Call Reduction**: From 70% to <1% call rate
- **Speedup**: 1.15x faster than baseline FBAM
- **Memory Efficiency**: Resolved CUDA OOM issues
- **Frame Timing**: Proper 60 FPS Game Boy emulation

### Generalization Achievements
- **Language-Agnostic**: Python + JavaScript code editing
- **Cross-Domain**: Code editing to game control
- **Real-World Data**: 352 developer commits, 2.16M repositories
- **Functional Metrics**: Diff correctness, test pass rates

### Architecture Benefits
- **Scalability**: O(1) memory lookups vs O(n) attention
- **Interpretability**: Symbolic memory is human-readable
- **Modularity**: Components can be swapped/upgraded
- **Efficiency**: Selective gating prevents unnecessary computation

## Current Implementation Status

### âœ… Completed
- Core SR-FBAM architecture with Transformer symbol extraction
- Cross-attention memory system
- Selective gating with hindsight supervision
- Pokemon Blue integration with PyBoy
- LLM integration with asynchronous planning
- CUDA memory optimization
- Proper Game Boy frame timing

### ðŸ”„ In Progress
- Intro sequence handling (currently stuck at TV/SNES)
- Dialogue loop resolution
- State transition robustness

### ðŸŽ¯ Next Steps
- Refine deterministic bootstrap for intro sequence
- Improve dialogue/menu state detection
- Enhance LLM decision triggers
- Expand to more complex game scenarios

## Technical Implementation Details

### Memory Operations
```python
# Discrete memory operations
ASSOC: Associate new information with existing context
FOLLOW: Retrieve related information from memory
WRITE: Persist new entities to long-term memory
HALT: Delegate to higher-level reasoning (LLM)
```

### Gating Logic
```python
# Selective gating decision
if gate_score > threshold:
    action = "ENCODE"  # Run expensive Transformer
else:
    action = "ASSOC"   # Reuse cached symbols
```

### LLM Integration
```python
# Asynchronous plan handling
plan = await llm.generate_plan(context)
execute_plan(plan, ttl=plan.duration)
```

## Engineering Considerations

### Performance
- **GPU Memory**: Eval mode prevents autograd graph buildup
- **CPU Efficiency**: Selective gating reduces unnecessary computation
- **I/O Optimization**: Asynchronous LLM calls prevent blocking

### Reliability
- **Error Handling**: Graceful degradation when LLM unavailable
- **State Recovery**: Robust state tracking across game transitions
- **Fallback Mechanisms**: Deterministic bootstrap for critical sequences

### Maintainability
- **Modular Design**: Clear separation of concerns
- **Configurable Parameters**: Tunable thresholds and timeouts
- **Comprehensive Logging**: Detailed telemetry for debugging

## Conclusion

SR-FBAM represents a significant advancement in long-horizon sequential task learning, combining the efficiency of symbolic memory with the flexibility of neural networks. The architecture successfully addresses traditional FBAM limitations while maintaining interpretability and scalability.

The current implementation demonstrates strong performance on code editing tasks and shows promise for complex game control scenarios, with ongoing work to resolve remaining edge cases in the Pokemon Blue intro sequence.
